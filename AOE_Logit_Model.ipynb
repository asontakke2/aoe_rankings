{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Empires 2 Player Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is used to rank invidividual players in team games. This model will allow us to better balance teams by calculating the probability that team wins before we actually play.\n",
    "\n",
    "Disclaimer: I am not a data scientist, who fully undertands the underlying math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Create test cases\n",
    "- Load data from Google Sheet instead of local CSV\n",
    "- Determine what EDA should be done\n",
    "- Fix GridSearchCV to LogisticRegression import\n",
    "- Explore adding a time component to factor in player improvement\n",
    "- Determine how to better input data for predicting\n",
    "- Build other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shaq</th>\n",
       "      <th>Gray</th>\n",
       "      <th>Rushi</th>\n",
       "      <th>Marc</th>\n",
       "      <th>Peter</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Ori</th>\n",
       "      <th>Vic</th>\n",
       "      <th>Ardy</th>\n",
       "      <th>Chad</th>\n",
       "      <th>Pat_Jr</th>\n",
       "      <th>Pat_Jr_Jr</th>\n",
       "      <th>Matt_M</th>\n",
       "      <th>Ben</th>\n",
       "      <th>Mikey</th>\n",
       "      <th>Evan</th>\n",
       "      <th>Medium_AI</th>\n",
       "      <th>Extra_Team</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Shaq  Gray  Rushi  Marc  Peter  Pat  Sam  Ori  Vic  Ardy  Chad  Pat_Jr  \\\n",
       "0     1     0     -1    -1      1    0   -1    0    0     0     0       0   \n",
       "1     1     0     -1     0     -1    1   -1    1    0     0     0       0   \n",
       "\n",
       "   Pat_Jr_Jr  Matt_M  Ben  Mikey  Evan  Medium_AI  Extra_Team  Outcome  \n",
       "0          0       0    0      0     0          0          -1       -1  \n",
       "1          0       0    0      0     0          0           0       -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "\n",
    "# Designate all columns that are not `Outcome` as features and `Outcome` as target\n",
    "X = df.loc[:, df.columns != 'Outcome']\n",
    "y = df.Outcome\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "This is where I should explore data. I haven't done any EDA since I created this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Normally, I would split the data into a training set and validation set. The validation set is for checking the accuracy of the best tuned model that results from cross-validation. HOWEVER, we are working with a really small dataset. Rather than hold out datafor validation, we will assess the performance of the model through the out of sample cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_validate,y_train,y_validate=train_test_split(X,y,test_size=0.33,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double data\n",
    "Since assigning teams is random, we want to ensure that the dataset is balanced. For example, when I record data, I generally always put myself as the home team (code as `1`). We mitigate this by not having an intercept term in our model. To be safe, we will still double the dataset by inverting all the records and concatenating to the orginal dataset.\n",
    "\n",
    "Doubling happens after splitting. Therefore we would need to double the training and validation sets. We use helper functions for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dataframe(original_dataframe):\n",
    "    \"\"\"Inverts the dataframe by simply multiplying all values by -1.\n",
    "\n",
    "    Args:\n",
    "        original_datatframe (df): The dataframe to be inverted.\n",
    "\n",
    "    Returns:\n",
    "        inverted_dataframe (df): The inverted dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    inverted_dataframe = original_dataframe.multiply(-1)\n",
    "    return inverted_dataframe\n",
    "\n",
    "def combine_dataframe(first_dataframe, second_dataframe):\n",
    "    \"\"\"Combines the dataframes. Assumes that both dataframes have the same columns\n",
    "\n",
    "    Args:\n",
    "        first_datatframe (df): The first dataframe to be combined.\n",
    "        second_datatframe (df): The second dataframe to be combined.\n",
    "\n",
    "    Returns:\n",
    "        combined_dataframe (df): The combined dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_dataframe = pd.concat([first_dataframe, second_dataframe])\n",
    "    return combined_dataframe\n",
    "\n",
    "def invert_and_combine(original_dataframe):\n",
    "    \"\"\"Inverts and combines the dataframes. Assumes that both dataframes have the same columns\n",
    "\n",
    "    Args:\n",
    "        original_dataframe (df): The dataframe to be inverted and combined with the original.\n",
    "\n",
    "    Returns:\n",
    "        new_dataframe (df): The combined dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    inverted_dataframe = invert_dataframe(original_dataframe)\n",
    "    new_dataframe = combine_dataframe(original_dataframe, inverted_dataframe)\n",
    "    return new_dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = invert_and_combine(X)\n",
    "y = invert_and_combine(y)\n",
    "\n",
    "# These are commented out because we are not using a validation set\n",
    "# X_train = invert_and_combine(X_train)\n",
    "# X_validate = invert_and_combine(X_validate)\n",
    "# y_train = invert_and_combine(y_train)\n",
    "# y_validate = invert_and_combine(y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "We will use 3 folds cross validation and GridSearch to determine the optimal hyper parameters for the logistic regression. The parameters we will search for is C (regularization and expressed as 1/lambda). We will assume `l2` penalty (default), `liblinear` solver (default), and no fit_intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "penalty = ['l2']\n",
    "solver = ['liblinear']\n",
    "fit_intercept = [False]\n",
    "param_grid = dict(C=C, penalty=penalty, fit_intercept=fit_intercept, solver=solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now execute the GridSearch over three folds. We will use `accuracy` to assess the performance of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "best_model_from_cv = grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect cross validation\n",
    "Here we inspect the accuracy and standard deviation for each run. Found code [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.539 (+/-0.070) for {'C': 0.001, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.559 (+/-0.058) for {'C': 0.01, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.624 (+/-0.187) for {'C': 0.1, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.773 (+/-0.108) for {'C': 1.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.838 (+/-0.077) for {'C': 10.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.838 (+/-0.099) for {'C': 100.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.838 (+/-0.099) for {'C': 1000.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.838 (+/-0.099) for {'C': 10000.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best model according to grid search: 0.84 using {'C': 100.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "means = best_model_from_cv.cv_results_['mean_test_score']\n",
    "stds = best_model_from_cv.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, best_model_from_cv.cv_results_['params']):\n",
    "    print(\"{0:.3f} (+/-{1:.3f}) for {2}\".format(mean, std * 2, params))\n",
    "    \n",
    "print(\"Best model according to grid search: {0} using {1}\".format(round(best_model_from_cv.best_score_,2), best_model_from_cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the right model\n",
    "There is a balance between model generalization and accuracy. Too low penalization (i.e. a high C) means that the model could be overfitting. However, low penalization means that the model has higher accuracy. Therefore, we will pick the model that has the lowest C, while still within the the bounds of the highest accuracy model. Specifically, we want the lowest C that is one standard error within the best C's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_accuracy(best_model_from_cv):\n",
    "    \"\"\"Finds the target accuracy that the second best model has to exceed\n",
    "\n",
    "    Args:\n",
    "        best_model_from_cv (GridSearchCV): The object that has the info from cross validation.\n",
    "\n",
    "    Returns:\n",
    "        target_accuracy (float): The best model's accuracy descreased by its standard deviation\n",
    "\n",
    "    \"\"\"\n",
    "    best_cv_model_index = best_model_from_cv.best_index_\n",
    "    best_cv_model_mean_accuracy = best_model_from_cv.cv_results_['mean_test_score'][best_cv_model_index]\n",
    "    best_cv_model_std_accuracy = best_model_from_cv.cv_results_['std_test_score'][best_cv_model_index]\n",
    "    target_accuracy = best_cv_model_mean_accuracy - best_cv_model_std_accuracy\n",
    "    return target_accuracy\n",
    "\n",
    "\n",
    "def find_final_model_params(best_model_from_cv):\n",
    "    \"\"\"Finds the parameters for the final model that will be trained on all data. \n",
    "    We want to see whether there is a model that has more generalization but satisfactory accuracy\n",
    "\n",
    "    Args:\n",
    "        best_model_from_cv (GridSearchCV): The object that has the info from cross validation.\n",
    "\n",
    "    Returns:\n",
    "        best_model_parameters (dict): The best model's parameters\n",
    "\n",
    "    \"\"\"\n",
    "    target_accuracy = find_target_accuracy(best_model_from_cv)\n",
    "    index_of_final_model = loop_through_cv_to_find_index_of_final_model(best_model_from_cv, target_accuracy)\n",
    "    best_model_parameters = best_model_from_cv.cv_results_['params'][index_of_final_model]\n",
    "    return best_model_parameters\n",
    "\n",
    "\n",
    "def loop_through_cv_to_find_index_of_final_model(best_model_from_cv, target_accuracy):\n",
    "    \"\"\"We want to see whether there is a model that has more generalization but satisfactory accuracy.\n",
    "    We loop through the results until we find a model that has a satisfactory accuracy.\n",
    "    The loop will stop at the best model in case none of the models with higher generalization are\n",
    "    satisfactory. This function is assuming that the models are sorted.\n",
    "\n",
    "    Args:\n",
    "        best_model_from_cv (GridSearchCV): The object that has the info from cross validation.\n",
    "        target_accuracy (float): The cv best model's accuracy descreased by its standard deviation\n",
    "\n",
    "    Returns:\n",
    "        target_index (int): The index of the final model\n",
    "\n",
    "    \"\"\"\n",
    "    target_index = best_model_from_cv.best_index_\n",
    "    for i, score in enumerate(best_model_from_cv.cv_results_['mean_test_score']):\n",
    "        if(score > target_accuracy and i < target_index):\n",
    "            target_index = i\n",
    "            print(\"Found better model: {0:.3f} (+/-{1:.3f}) using {2}\".format(best_model_from_cv.cv_results_['mean_test_score'][target_index],\n",
    "                                                                      2*best_model_from_cv.cv_results_['std_test_score'][target_index],\n",
    "                                                                      best_model_from_cv.cv_results_['params'][target_index]))\n",
    "            break\n",
    "    return target_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found better model: 0.838 (+/-0.077) using {'C': 10.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "best_model_parameters = find_final_model_params(best_model_from_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess performance\n",
    "Normally, we would now use the tuned hyperparameters to ensure accuracy on the validation set. As a reminder, the model is trained on the training set and the scores are computed on the validation set. HOWEVER, as mentioned, we would rather use all of our limited data for model building, so we will not assess performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model_with_only_training_data = = LogisticRegression(penalty=best_model_parameters['penalty'], \n",
    "#                                  C=best_model_parameters['C'],\n",
    "#                                  fit_intercept=best_model_parameters['fit_intercept'],\n",
    "#                                  solver=best_model_parameters['solver'])\n",
    "\n",
    "# final_model_with_only_training_data = final_model_with_only_training_data.fit(X, y)\n",
    "# y_true, y_pred = y_validate, final_model_with_only_training_data.predict(X_validate)\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a ROC curve to visualize performance. The more above the diagonal, the better. More info [here](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_roc_auc = roc_auc_score(y_validate, final_model_with_only_training_data.predict(X_validate))\n",
    "# fpr, tpr, thresholds = roc_curve(y_validate, final_model_with_only_training_data.predict_proba(X_validate)[:,1])\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final model\n",
    "Since we are happy with the performance of our model on the validation set, we will re-fit it with all the data. There is no concern of overfitting because we already validated against data the model hadn't seen. Since we removed the validation set, the final model is the same as the model that resulted from the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: must be a cleaner way to import GridSearchCV into LogisticRegression\n",
    "final_model_with_all_data = LogisticRegression(penalty=best_model_parameters['penalty'], \n",
    "                                 C=best_model_parameters['C'],\n",
    "                                 fit_intercept=best_model_parameters['fit_intercept'],\n",
    "                                 solver=best_model_parameters['solver'])\n",
    "\n",
    "final_model_with_all_data = final_model_with_all_data.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will output the final coefficients to see how players are ranked and with what magnitude. We need to do some busy work to output a list of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model coefficients are: [(-2.8, 'Mikey'), (-2.67, 'Chad'), (-2.53, 'Sam'), (-1.92, 'Ori'), (-1.81, 'Matt_M'), (-1.67, 'Marc'), (-1.23, 'Evan'), (-0.3, 'Ben'), (-0.02, 'Pat_Jr_Jr'), (0.14, 'Medium_AI'), (0.98, 'Pat'), (1.13, 'Pat_Jr'), (1.35, 'Peter'), (1.44, 'Shaq'), (1.52, 'Ardy'), (2.74, 'Gray'), (2.92, 'Extra_Team'), (3.79, 'Vic'), (4.61, 'Rushi')]\n"
     ]
    }
   ],
   "source": [
    "features = list(df.columns)\n",
    "features.remove(\"Outcome\")\n",
    "[coef] = final_model_with_all_data.coef_.tolist()\n",
    "\n",
    "rounded_coef = []\n",
    "for number in coef:\n",
    "    rounded_number = round(number, 2)\n",
    "    rounded_coef.append(rounded_number)\n",
    "\n",
    "x = zip(rounded_coef, features)\n",
    "print(\"Final model coefficients are: {0}\".format(sorted(list(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Ultimately, we want to use this model to determine the probability of a game. Each value in the array corresponds to a person. For example, the first number is Shaq, the second number is Gray, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probability that Marc (-1) beats Rushi (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc beats Rushi is 0.19%\n"
     ]
    }
   ],
   "source": [
    "result = final_model_with_all_data.predict_proba([[0, 0, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "print(\"The probability that Marc beats Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probability that Shaq (-1) beats Gray (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Shaq beats Gray is 21.43%\n"
     ]
    }
   ],
   "source": [
    "result = final_model_with_all_data.predict_proba([[-1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "print(\"The probability that Shaq beats Gray is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Shaq (-1) and Gray (-1) beat Rushi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Shaq and Gray beat Rushi is 92.4%\n"
     ]
    }
   ],
   "source": [
    "result = final_model_with_all_data.predict_proba([[-1, -1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]])\n",
    "print(\"The probability that Shaq and Gray beat Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Marc (-1) beats Sam (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc beats Sam is 70.31%\n"
     ]
    }
   ],
   "source": [
    "result = final_model_with_all_data.predict_proba([[0, 0, 0, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "print(\"The probability that Marc beats Sam is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Marc (-1) and Sam (-1) beat Rushi (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc and Sam beat Rushi is 0.27%\n"
     ]
    }
   ],
   "source": [
    "result = final_model_with_all_data.predict_proba([[0, 0, 1, -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1]])\n",
    "print(\"The probability that Marc and Sam beat Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Vic (-1) beats Rushi (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Vic beats Rushi is 30.72%\n"
     ]
    }
   ],
   "source": [
    "result = final_model_with_all_data.predict_proba([[0, 0, 1, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "print(\"The probability that Vic beats Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
