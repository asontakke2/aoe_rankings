{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Empires 2 Player Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is used to rank invidividual players in team games. This model will allow us to better balance teams by calculating the probability that team wins before we actually play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Choose the best C that balances stricter regularization and adequate accuracy\n",
    "- Do we need to manually derive the cost function? Could write a custom scoring function for GridSearchCV but I don't know what we're doing\n",
    "- Load data from Google Sheet instead of local CSV\n",
    "- Determine what EDA should be done\n",
    "- Fix GridSearchCV to LogisticRegression import\n",
    "- Explore adding a time component to factor in player improvement\n",
    "- Determine how to better input data for predicting\n",
    "- Build other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shaq</th>\n",
       "      <th>Gray</th>\n",
       "      <th>Rushi</th>\n",
       "      <th>Marc</th>\n",
       "      <th>Peter</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Ori</th>\n",
       "      <th>Vic</th>\n",
       "      <th>Ardy</th>\n",
       "      <th>Chad</th>\n",
       "      <th>Pat_Jr</th>\n",
       "      <th>Pat_Jr_Jr</th>\n",
       "      <th>Matt_M</th>\n",
       "      <th>Ben</th>\n",
       "      <th>Mikey</th>\n",
       "      <th>Evan</th>\n",
       "      <th>Medium_AI</th>\n",
       "      <th>Extra_Team</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Shaq  Gray  Rushi  Marc  Peter  Pat  Sam  Ori  Vic  Ardy  Chad  Pat_Jr  \\\n",
       "0     1     0     -1    -1      1    0   -1    0    0     0     0       0   \n",
       "1     1     0     -1     0     -1    1   -1    1    0     0     0       0   \n",
       "\n",
       "   Pat_Jr_Jr  Matt_M  Ben  Mikey  Evan  Medium_AI  Extra_Team  Outcome  \n",
       "0          0       0    0      0     0          0          -1       -1  \n",
       "1          0       0    0      0     0          0           0       -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"sample_data.csv\")\n",
    "\n",
    "# Designate all columns that are not `Outcome` as features and `Outcome` as target\n",
    "X = df.loc[:, df.columns != 'Outcome']\n",
    "y = df.Outcome\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "This is where I should explore data. I haven't done any EDA since I created this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Normally, I would split the data into a training set and validation set. The validation set is for checking the accuracy of the best tuned model that results from cross-validation. HOWEVER, we are working with a really small dataset. Rather than hold out datafor validation, we will assess the performance of the model through the out of sample cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_validate,y_train,y_validate=train_test_split(X,y,test_size=0.33,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double data\n",
    "Since assigning teams is random, we want to ensure that the dataset is balanced. For example, when I record data, I generally always put myself as the home team (code as `1`). We mitigate this by not having an intercept term in our model. To be safe, we will still double the dataset by inverting all the records and concatenating to the orginal dataset.\n",
    "\n",
    "Doubling happens after splitting. Therefore we would need to double the training and validation sets. We use helper functions for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_dataframe(original_dataframe):\n",
    "    \"\"\"Inverts the dataframe by simply multiplying all values by -1.\n",
    "\n",
    "    Args:\n",
    "        original_datatframe (df): The dataframe to be inverted.\n",
    "\n",
    "    Returns:\n",
    "        inverted_dataframe (df): The inverted dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    inverted_dataframe = original_dataframe.multiply(-1)\n",
    "    return(inverted_dataframe)\n",
    "\n",
    "def combine_dataframe(first_dataframe, second_dataframe):\n",
    "    \"\"\"Combines the dataframes. Assumes that both dataframes have the same columns\n",
    "\n",
    "    Args:\n",
    "        first_datatframe (df): The first dataframe to be combined.\n",
    "        second_datatframe (df): The second dataframe to be combined.\n",
    "\n",
    "    Returns:\n",
    "        combined_dataframe (df): The combined dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    combined_dataframe = pd.concat([first_dataframe, second_dataframe])\n",
    "    return(combined_dataframe)\n",
    "\n",
    "def invert_and_combine(original_dataframe):\n",
    "    \"\"\"Inverts and combines the dataframes. Assumes that both dataframes have the same columns\n",
    "\n",
    "    Args:\n",
    "        original_dataframe (df): The dataframe to be inverted and combined with the original.\n",
    "\n",
    "    Returns:\n",
    "        new_dataframe (df): The combined dataframe.\n",
    "\n",
    "    \"\"\"\n",
    "    inverted_dataframe = invert_dataframe(original_dataframe)\n",
    "    new_dataframe = combine_dataframe(original_dataframe, inverted_dataframe)\n",
    "    return(new_dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = invert_and_combine(X)\n",
    "y = invert_and_combine(y)\n",
    "\n",
    "# These are commented out because we are not using a validation set\n",
    "# X_train = invert_and_combine(X_train)\n",
    "# X_validate = invert_and_combine(X_validate)\n",
    "# y_train = invert_and_combine(y_train)\n",
    "# y_validate = invert_and_combine(y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "We will use 5 folds cross validation and GridSearch to determine the optimal hyper parameters for the logistic regression. The parameters we will search for is C (regularization and expressed as 1/lambda). We will assume `l2` penalty (default), `liblinear` solver (default), and no fit_intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "penalty = ['l2']\n",
    "solver = ['liblinear']\n",
    "fit_intercept=[False]\n",
    "param_grid = dict(C=C, penalty=penalty, fit_intercept=fit_intercept, solver=solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now execute the GridSearch over three folds. We will use `accuracy` to assess the performance of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.82 using {'C': 100.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv = 5, n_jobs=-1, scoring='accuracy')\n",
    "best_model = grid.fit(X, y)\n",
    "\n",
    "print(\"Best score: {0} using {1}\".format(round(best_model.best_score_,2), best_model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect cross validation\n",
    "Here we inspect the accuracy and standard deviation for each run. Lutes says we should \"choose the C that gives you one standard error under the best cost function\". However, I don't know how to get the cost function. Do we need to write a custom scoring function for GridSearchCV so that it isn't selecting the CV that had the best `accuracy`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573 (+/-0.261) for {'C': 0.001, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.579 (+/-0.307) for {'C': 0.01, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.651 (+/-0.352) for {'C': 0.1, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.780 (+/-0.224) for {'C': 1.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.812 (+/-0.093) for {'C': 10.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.818 (+/-0.086) for {'C': 100.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.818 (+/-0.086) for {'C': 1000.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "0.818 (+/-0.086) for {'C': 10000.0, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\n",
    "means = best_model.cv_results_['mean_test_score']\n",
    "stds = best_model.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, best_model.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "          % (mean, std * 2, params))\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess performance\n",
    "Normally, we would now use the tuned hyperparameters to ensure accuracy on the validation set. As a reminder, the model is trained on the training set and the scores are computed on the validation set. HOWEVER, as mentioned, we would rather use all of our limited data for model building, so we will not assess performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true, y_pred = y_validate, best_model.predict(X_validate)\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a ROC curve to visualize performance. The more above the diagonal, the better. More info [here](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_roc_auc = roc_auc_score(y_validate, best_model.predict(X_validate))\n",
    "# fpr, tpr, thresholds = roc_curve(y_validate, best_model.predict_proba(X_validate)[:,1])\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final model\n",
    "Since we are happy with the performance of our model on the validation set, we will re-fit it with all the data. There is no concern of overfitting because we already validated against data the model hadn't seen. Since we removed the validation set, the final model is the same as the model that resulted from the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: must be a cleaner way to import GridSearchCV into LogisticRegression\n",
    "final_model = LogisticRegression(penalty=best_model.best_params_['penalty'], \n",
    "                                 C=best_model.best_params_['C'],\n",
    "                                 fit_intercept=best_model.best_params_['fit_intercept'],\n",
    "                                 solver=best_model.best_params_['solver'])\n",
    "\n",
    "final_model = final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will output the final coefficients to see how players are ranked and with what magnitude. We need to do some busy work to output a list of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-7.03, 'Mikey'), (-3.39, 'Chad'), (-2.78, 'Sam'), (-2.35, 'Matt_M'), (-2.11, 'Ori'), (-1.92, 'Evan'), (-1.85, 'Ben'), (-1.79, 'Marc'), (0.14, 'Medium_AI'), (0.41, 'Pat_Jr_Jr'), (1.5, 'Pat'), (1.86, 'Pat_Jr'), (2.03, 'Shaq'), (2.11, 'Peter'), (2.17, 'Ardy'), (3.44, 'Extra_Team'), (3.84, 'Gray'), (5.1, 'Vic'), (6.07, 'Rushi')]\n"
     ]
    }
   ],
   "source": [
    "features = list(df.columns)\n",
    "features.remove(\"Outcome\")\n",
    "[coef] = final_model.coef_.tolist()\n",
    "\n",
    "rounded_coef = []\n",
    "for number in coef:\n",
    "    rounded_number = round(number, 2)\n",
    "    rounded_coef.append(rounded_number)\n",
    "\n",
    "x = zip(rounded_coef, features)\n",
    "print(sorted(list(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Ultimately, we want to use this model to determine the probability of a game. Each value in the array corresponds to a person. For example, the first number is Shaq, the second number is Gray, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probability that Marc (-1) beats Rushi (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc beats Rushi is 0.04%\n"
     ]
    }
   ],
   "source": [
    "result = final_model.predict_proba([[0,0,1,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "print(\"The probability that Marc beats Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probability that Shaq (-1) beats Gray (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Shaq beats Gray is 14.17%\n"
     ]
    }
   ],
   "source": [
    "result = final_model.predict_proba([[-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "print(\"The probability that Shaq beats Gray is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Shaq (-1) and Gray (-1) beat Rushi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Shaq and Gray beat Rushi is 96.21%\n"
     ]
    }
   ],
   "source": [
    "result = final_model.predict_proba([[-1,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1]])\n",
    "\n",
    "print(\"The probability that Shaq and Gray beat Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Marc (-1) beats Sam (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc beats Sam is 72.95%\n"
     ]
    }
   ],
   "source": [
    "result = final_model.predict_proba([[0,0,0,-1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "print(\"The probability that Marc beats Sam is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Marc (-1) and Sam (-1) beat Rushi (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc and Sam beat Rushi is 0.07%\n"
     ]
    }
   ],
   "source": [
    "result = final_model.predict_proba([[0,0,1,-1,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,-1]])\n",
    "\n",
    "print(\"The probability that Marc and Sam beat Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have modeled the probaility that Vic (-1) beats Rushi (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Vic beats Rushi is 27.42%\n"
     ]
    }
   ],
   "source": [
    "result = final_model.predict_proba([[0,0,1,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "print(\"The probability that Vic beats Rushi is {0}%\".format(round(result[0][0]*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
