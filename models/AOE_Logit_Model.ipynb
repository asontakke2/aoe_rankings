{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age of Empires 2 Player Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is used to rank invidividual players in team games. This model will allow us to better balance teams by calculating the probability that team wins before we actually play.\n",
    "\n",
    "Disclaimer: I am not a data scientist, who fully undertands the underlying math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo:\n",
    "- Create test cases\n",
    "- Load data from Google Sheet instead of local CSV\n",
    "- Determine what EDA should be done\n",
    "- Fix GridSearchCV to LogisticRegression import\n",
    "- Explore adding a time component to factor in player improvement\n",
    "- Determine how to better input data for predicting\n",
    "- Build other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from utilities.predictions import make_predictions\n",
    "from utilities.transformations import invert_and_combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shaq</th>\n",
       "      <th>Gray</th>\n",
       "      <th>Rushi</th>\n",
       "      <th>Marc</th>\n",
       "      <th>Peter</th>\n",
       "      <th>Pat</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Ori</th>\n",
       "      <th>Vic</th>\n",
       "      <th>Ardy</th>\n",
       "      <th>Chad</th>\n",
       "      <th>Pat_Jr</th>\n",
       "      <th>Rory</th>\n",
       "      <th>Matt_M</th>\n",
       "      <th>Ben</th>\n",
       "      <th>Mikey</th>\n",
       "      <th>Evan</th>\n",
       "      <th>Medium_AI</th>\n",
       "      <th>Extra_Team</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Shaq  Gray  Rushi  Marc  Peter  Pat  Sam  Ori  Vic  Ardy  Chad  Pat_Jr  \\\n",
       "0     1     0     -1    -1      1    0   -1    0    0     0     0       0   \n",
       "1     1     0     -1     0     -1    1   -1    1    0     0     0       0   \n",
       "\n",
       "   Rory  Matt_M  Ben  Mikey  Evan  Medium_AI  Extra_Team  Outcome  \n",
       "0     0       0    0      0     0          0          -1       -1  \n",
       "1     0       0    0      0     0          0           0       -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/sample_data.csv\")\n",
    "\n",
    "# Designate all columns that are not `Outcome` as features and `Outcome` as target\n",
    "X = df.loc[:, df.columns != 'Outcome']\n",
    "y = df.Outcome\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data\n",
    "This is where I should explore data. I haven't done any EDA since I created this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Normally, I would split the data into a training set and validation set. The validation set is for checking the accuracy of the best tuned model that results from cross-validation. HOWEVER, we are working with a really small dataset. Rather than hold out datafor validation, we will assess the performance of the model through the out of sample cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train,X_validate,y_train,y_validate=train_test_split(X,y,test_size=0.33,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double data\n",
    "Since assigning teams is random, we want to ensure that the dataset is balanced. For example, when I record data, I generally always put myself as the home team (code as `1`). We mitigate this by not having an intercept term in our model. To be safe, we will still double the dataset by inverting all the records and concatenating to the orginal dataset.\n",
    "\n",
    "Doubling happens after splitting. Therefore we would need to double the training and validation sets. We use helper functions for readability. The functions are in a utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = invert_and_combine(X)\n",
    "y = invert_and_combine(y)\n",
    "\n",
    "# These are commented out because we are not using a validation set\n",
    "# X_train = invert_and_combine(X_train)\n",
    "# X_validate = invert_and_combine(X_validate)\n",
    "# y_train = invert_and_combine(y_train)\n",
    "# y_validate = invert_and_combine(y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "We will use 3 folds cross validation and GridSearch to determine the optimal hyper parameters for the logistic regression. The parameters we will search for is C (regularization and expressed as 1/lambda). We will assume `l2` penalty (default), `liblinear` solver (default), and no fit_intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.logspace(-1,4,1000)\n",
    "penalty = ['l2']\n",
    "solver = ['liblinear']\n",
    "fit_intercept = [False]\n",
    "param_grid = dict(C=C, penalty=penalty, fit_intercept=fit_intercept, solver=solver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now execute the GridSearch over three folds. We will use `accuracy` to assess the performance of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model according to grid search: 0.82 using {'C': 14.865248449978571, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "grid = GridSearchCV(estimator=lr, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "cross_validation_models = grid.fit(X, y)\n",
    "\n",
    "print(\"Best model according to grid search: {0} using {1}\".format(\n",
    "    round(cross_validation_models.best_score_, 2), cross_validation_models.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking the right model\n",
    "There is a balance between model generalization and accuracy. Too low penalization (i.e. a high C) means that the model could be overfitting. However, low penalization means that the model has higher accuracy. Therefore, we will pick the model that has the lowest C, while still within the the bounds of the highest accuracy model. Specifically, we want the lowest C that is one standard error within the best C's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_accuracy(cv_models):\n",
    "    \"\"\"Finds the target accuracy that the second best model has to exceed\n",
    "\n",
    "    Args:\n",
    "        cv_models (GridSearchCV): The object that has the info from cross validation.\n",
    "\n",
    "    Returns:\n",
    "        target_accuracy (float): The best model's accuracy descreased by its standard deviation\n",
    "\n",
    "    \"\"\"\n",
    "    best_cv_model_index = cv_models.best_index_\n",
    "    best_cv_model_mean_accuracy = cv_models.cv_results_['mean_test_score'][best_cv_model_index]\n",
    "    best_cv_model_std_accuracy = cv_models.cv_results_['std_test_score'][best_cv_model_index]\n",
    "    target_accuracy = best_cv_model_mean_accuracy - best_cv_model_std_accuracy\n",
    "    return target_accuracy\n",
    "\n",
    "\n",
    "def find_final_model_params(cv_models):\n",
    "    \"\"\"Finds the parameters for the final model that will be trained on all data.\n",
    "    We want to see whether there is a model that has more generalization but satisfactory accuracy\n",
    "\n",
    "    Args:\n",
    "        cv_models (GridSearchCV): The object that has the info from cross validation.\n",
    "\n",
    "    Returns:\n",
    "        final_model_params (dict): The final model's parameters\n",
    "\n",
    "    \"\"\"\n",
    "    target_accuracy = find_target_accuracy(cv_models)\n",
    "    index_of_final_model = loop_through_cv_to_find_index_of_final_model(cv_models, target_accuracy)\n",
    "    final_model_params = cv_models.cv_results_['params'][index_of_final_model]\n",
    "    return final_model_params\n",
    "\n",
    "\n",
    "def loop_through_cv_to_find_index_of_final_model(cv_models, target_accuracy):\n",
    "    \"\"\"We want to see whether there is a model that has more generalization but\n",
    "    satisfactory accuracy. We loop through the results until we find a model\n",
    "    that has a satisfactory accuracy. The loop will stop at the best model in\n",
    "    case none of the models with higher generalization are satisfactory.\n",
    "    This function is assuming that the models are sorted.\n",
    "\n",
    "    Args:\n",
    "        cv_models (GridSearchCV): The object that has the info from cross validation.\n",
    "        target_accuracy (float): The cv best model's accuracy descreased by its standard deviation\n",
    "\n",
    "    Returns:\n",
    "        target_index (int): The index of the final model\n",
    "\n",
    "    \"\"\"\n",
    "    target_index = cv_models.best_index_\n",
    "    for i, score in enumerate(cv_models.cv_results_['mean_test_score']):\n",
    "        if score > target_accuracy and i < target_index:\n",
    "            target_index = i\n",
    "            print(\"Found adequate model with better generalization: {0:.3f} (+/-{1:.3f}) using {2}\".\n",
    "                  format(cv_models.cv_results_['mean_test_score'][target_index],\n",
    "                         2 * cv_models.cv_results_['std_test_score'][target_index],\n",
    "                         cv_models.cv_results_['params'][target_index]))\n",
    "            break\n",
    "    return target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found adequate model with better generalization: 0.793 (+/-0.059) using {'C': 1.867545842761076, 'fit_intercept': False, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "best_model_parameters = find_final_model_params(cross_validation_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess performance\n",
    "Normally, we would now use the tuned hyperparameters to ensure accuracy on the validation set. As a reminder, the model is trained on the training set and the scores are computed on the validation set. HOWEVER, as mentioned, we would rather use all of our limited data for model building, so we will not assess performance on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score, roc_curve, classification_report\n",
    "# import matplotlib.pyplot as plt\n",
    "# final_model_with_only_training_data = LogisticRegression(penalty=best_model_parameters['penalty'], \n",
    "#                                  C=best_model_parameters['C'],\n",
    "#                                  fit_intercept=best_model_parameters['fit_intercept'],\n",
    "#                                  solver=best_model_parameters['solver'])\n",
    "\n",
    "# final_model_with_only_training_data = final_model_with_only_training_data.fit(X, y)\n",
    "# y_true, y_pred = y_validate, final_model_with_only_training_data.predict(X_validate)\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a ROC curve to visualize performance. The more above the diagonal, the better. More info [here](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_roc_auc = roc_auc_score(y_validate, final_model_with_only_training_data.predict(X_validate))\n",
    "# fpr, tpr, thresholds = roc_curve(y_validate, final_model_with_only_training_data.predict_proba(X_validate)[:,1])\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final model\n",
    "Since we are happy with the performance of our model on the validation set, we will re-fit it with all the data. There is no concern of overfitting because we already validated against data the model hadn't seen. Since we removed the validation set, the final model is the same as the model that resulted from the hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: must be a cleaner way to import GridSearchCV into LogisticRegression\n",
    "final_model_with_all_data = LogisticRegression(penalty=best_model_parameters['penalty'],\n",
    "                                               C=best_model_parameters['C'],\n",
    "                                               fit_intercept=best_model_parameters['fit_intercept'],\n",
    "                                               solver=best_model_parameters['solver'])\n",
    "\n",
    "final_model_with_all_data = final_model_with_all_data.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will output the final coefficients to see how players are ranked and with what magnitude. We need to do some busy work to output a list of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model coefficients are: [(-1.82, 'Sam'), (-1.44, 'Chad'), (-1.42, 'Ori'), (-1.1, 'Marc'), (-0.72, 'Rory'), (-0.7, 'Mikey'), (-0.35, 'Matt_M'), (-0.19, 'Evan'), (-0.05, 'Ben'), (0.34, 'Medium_AI'), (0.48, 'Pat_Jr'), (0.54, 'Pat'), (0.62, 'Peter'), (0.76, 'Shaq'), (1.1, 'Ardy'), (1.36, 'Gray'), (1.91, 'Vic'), (2.07, 'Extra_Team'), (2.68, 'Rushi')]\n"
     ]
    }
   ],
   "source": [
    "features = list(df.columns)\n",
    "features.remove(\"Outcome\")\n",
    "[coef] = final_model_with_all_data.coef_.tolist()\n",
    "\n",
    "rounded_coef = []\n",
    "for number in coef:\n",
    "    rounded_number = round(number, 2)\n",
    "    rounded_coef.append(rounded_number)\n",
    "\n",
    "x = zip(rounded_coef, features)\n",
    "print(\"Final model coefficients are: {0}\".format(sorted(list(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Ultimately, we want to use this model to determine the probability of a game. Each value in the array corresponds to a person. For example, the first number is Shaq, the second number is Gray, etc. We use a utility function to compute the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that Marc beats Rushi is 2.23%\n",
      "The probability that Shaq beats Gray is 35.55%\n",
      "The probability that Shaq and Gray beat Rushi is 82.0%\n",
      "The probability that Marc beats Sam is 67.1%\n",
      "The probability that Marc and Sam beat Rushi is 2.86%\n",
      "The probability that Vic beats Rushi is 31.64%\n"
     ]
    }
   ],
   "source": [
    "make_predictions(final_model_with_all_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
